<!DOCTYPE html>
<html lang="cn-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="最近在做机器翻译相关的工作，发现subword算法在NLP各大任务中无处不在。既然要用到并且用好subword，这里就重点捋一遍关于subword的算法以及几个开源的实现。 1.word、subword和character在神经机器翻译中，通常有一个固定的词表，并且模型的训练和预测都非常依赖这个词表。在神经网络的训练过程中，需要对词表中每个词做向量表，每个词对应不同的向量，即embedding的过">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP中的subword算法及实现">
<meta property="og:url" content="http://example.com/2019/11/18/NLP%E4%B8%AD%E7%9A%84subword%E7%AE%97%E6%B3%95/index.html">
<meta property="og:site_name" content="grovlyhop&#39;s blog">
<meta property="og:description" content="最近在做机器翻译相关的工作，发现subword算法在NLP各大任务中无处不在。既然要用到并且用好subword，这里就重点捋一遍关于subword的算法以及几个开源的实现。 1.word、subword和character在神经机器翻译中，通常有一个固定的词表，并且模型的训练和预测都非常依赖这个词表。在神经网络的训练过程中，需要对词表中每个词做向量表，每个词对应不同的向量，即embedding的过">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/2019/11/18/NLP%E4%B8%AD%E7%9A%84subword%E7%AE%97%E6%B3%95/wordpiece1.jpg">
<meta property="og:image" content="http://example.com/2019/11/18/NLP%E4%B8%AD%E7%9A%84subword%E7%AE%97%E6%B3%95/wordpiece2.jpg">
<meta property="article:published_time" content="2019-11-17T16:00:00.000Z">
<meta property="article:modified_time" content="2020-03-14T16:00:00.000Z">
<meta property="article:author" content="zhangjf">
<meta property="article:tag" content="translation">
<meta property="article:tag" content="subword">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2019/11/18/NLP%E4%B8%AD%E7%9A%84subword%E7%AE%97%E6%B3%95/wordpiece1.jpg">

<link rel="canonical" href="http://example.com/2019/11/18/NLP%E4%B8%AD%E7%9A%84subword%E7%AE%97%E6%B3%95/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'cn-Hans'
  };
</script>

  <title>NLP中的subword算法及实现 | grovlyhop's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">grovlyhop's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="cn-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/11/18/NLP%E4%B8%AD%E7%9A%84subword%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="zhangjf">
      <meta itemprop="description" content="Even night drunk with you for real.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="grovlyhop's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          NLP中的subword算法及实现
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-18 00:00:00" itemprop="dateCreated datePublished" datetime="2019-11-18T00:00:00+08:00">2019-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-15 00:00:00" itemprop="dateModified" datetime="2020-03-15T00:00:00+08:00">2020-03-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Nature-Language-Processing/" itemprop="url" rel="index"><span itemprop="name">Nature Language Processing</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>5.8k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>5 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>最近在做机器翻译相关的工作，发现subword算法在NLP各大任务中无处不在。既然要用到并且用好subword，这里就重点捋一遍关于subword的算法以及几个开源的实现。</p>
<h1 id="1-word、subword和character"><a href="#1-word、subword和character" class="headerlink" title="1.word、subword和character"></a>1.word、subword和character</h1><p>在神经机器翻译中，通常有一个固定的词表，并且模型的训练和预测都非常依赖这个词表。在神经网络的训练过程中，需要对词表中每个词做向量表，每个词对应不同的向量，即embedding的过程。每个不同的词对应不同的向量，即使两个词看起来十分相近，但在训练过程词向量没有任何关系。这就导致一个单词因为拥有不同的形态产生不同的词，从而产生大词汇量的问题。</p>
<p>机器翻译的词表是定长的，但是需要实际翻译的词汇是开放的(out of vocabulary)。以前的做法是新词汇添加到词典中，但是过大的词典会带来两个问题：</p>
<ul>
<li>稀疏问题: 某些词汇出现的频率很低，得不到充分的训练</li>
<li>计算量问题: 词典过大，也就意味着embedding过程的计算量会变大</li>
</ul>
<p>同时，这种word-level的处理方式并不能通过增大词表真正解决OOV的问题，因为再大的词典不能真正覆盖所有的词汇。</p>
<p>为了处理这个问题，一个思路是将字符当做基本单元，建立character-level模型。character-level模型试图使用26个字母加上一些符号去表示所有的词汇，相比于word-level模型，这种处理方式的粒度变小，其输入长度变长，使得数据更加稀疏并且难以学习长远程的依赖关系。类似的工作可参考<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1610.03017">Character-Level Neural Machine Translation</a>，实验结论是基于字符的模型能更好处理OOV问题，并且能更好学习多语言之间通用的语素。</p>
<p>word-level模型导致严重的OOV，而character-level模型粒度又太小，那么subword-level的处理方式就应运而生。subword将单词划分为更小的单元，比如”older”划分为”old” 和 “er”，而这些单元往往能应用到别的词汇当中。举个例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">训练集的词汇: old older oldest smart smarter smartest</span><br><span class="line">word-level 词典: old older oldest smart smarter smartest 长度为6</span><br><span class="line">subword-level 词典: old smart er est 长度为4</span><br></pre></td></tr></table></figure>
<p>将词划分成字词的形式，能够大大降低词典的大小。同时，未知词汇能以subword组合的形式表示出来，也能提升词典的表达能力。</p>
<h1 id="2-3种subword算法"><a href="#2-3种subword算法" class="headerlink" title="2. 3种subword算法"></a>2. 3种subword算法</h1><p>如何将词分解成subword，依据不同的策略，产生了几种主流的方法: <a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/P16-1162/">Byte Pair Encoding (BPE)</a>、<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1609.08144.pdf">wordpiece</a> 和 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.10959">Unigram Language Model</a>。值得一提的是，这几种算法的处理流程跟语言学没有太大的关系，单纯是统计学的解决思路。</p>
<h2 id="2-1-Byte-Pair-Encoding"><a href="#2-1-Byte-Pair-Encoding" class="headerlink" title="2.1 Byte Pair Encoding"></a>2.1 Byte Pair Encoding</h2><p>Byte Pair Encoding (BPE) 是一种压缩算法，它属于自下而上的算法。BPE最早应用于NLP任务来源于<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.07909">Neural Machine Translation of Rare Words with Subword Units</a>，目前已经是NLP任务中最为简单有效的方法。<a target="_blank" rel="noopener" href="https://github.com/openai/gpt-2">GPT-2</a>中使用BPE作为subword算法，并且WMT比赛中多数提交者使用的subword算法也是BPE。</p>
<p>BPE是一种数据压缩的方式，它将字符串中最常见的一对连续字符数据替换成该字符串中不存在的字符串，后续再通过一个词表重建原始的数据。BPE的处理过程可以理解为一个单词的再拆分过程。如”loved”,”loving”,”loves”这三个单词，其本身的语义都是”爱”的意思。BPE通过训练，能够把上面的3个单词拆分成”lov”,”ed”,”ing”,”es”几部分，这样可以把词的本身的意思和时态分开，有效的减少了词表的数量。</p>
<p>BPE算法的流程可参考<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.07909">论文</a>，这里添加一些自己的理解。</p>
<ul>
<li><p>获取subword词表的流程(learn-bpe)</p>
<ul>
<li>准备语料，分解成最小单元，比如英文中26个字母加上各种符号，作为原始词表</li>
<li>根据语料统计相邻字符对出现的频次</li>
<li>挑出频次最高的相邻字符对，比如”t”和”h”，合并组成”th”，加入词表，训练语料中所有该相邻字符对都进行融合</li>
<li>重复2和3操作，直至词表中单词的数量达到期望，或下一个最高频的字节对出现频率为1</li>
</ul>
</li>
<li><p>编码和解码(apply-bpe及其逆过程)</p>
<ul>
<li><strong>编码</strong>: 得到subword词表后，对该词表按照子词长度由大到小排序。编码时，对于每个单词，遍历排好序的字词词表寻找是否有token是当前单词的子字符串，如果有，则该token是表示单词的tokens之一。从最长的token迭代到最短的token，尝试将每个单词中的子字符串替换为token。 最终，该过程将迭代所有tokens，并将所有子字符串替换为tokens。 如果仍然有子字符串没被替换但所有token都已迭代完毕，则将剩余的子词替换为特殊token，如<code>&lt;unk&gt;</code>。</li>
<li><strong>解码</strong>: 该过程是编码的逆过程，主要应用在得到翻译输出怎么将subword恢复为word。使用简单的符号替换即可:  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -r &#x27;s/(@@ )|(@@ ?$)//g&#x27;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2 id="2-2-wordpiece"><a href="#2-2-wordpiece" class="headerlink" title="2.2 wordpiece"></a>2.2 wordpiece</h2><p>wordpiece作为<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">Bert</a>使用的分词方式，其生成词表的方式和BPE非常相近，都是用合并token的方式来生成新的token，最大的区别在于选择合并哪两个token。BPE选择频率最高的相邻字符对进行合并，而wordpiece是基于概率生成的，参考按照作者的原话:</p>
<blockquote>
<p>Choose the new word unit out of all possible ones that increase the likelihood on the training data the most when added to the mode.</p>
</blockquote>
<p>从字面上可能有些难以理解，列一下公式就比较清楚了。在做分词的时候假设词和词之间是独立的，所以句子的likelihood等于句子中每个词概率的乘积：</p>
<p><img src="/2019/11/18/NLP%E4%B8%AD%E7%9A%84subword%E7%AE%97%E6%B3%95/wordpiece1.jpg"></p>
<p>如果把相邻的x和y两个token合并生成一个新的token叫做z，那么整个句子likelihood的变化可以用下面的式子来表达：</p>
<p><img src="/2019/11/18/NLP%E4%B8%AD%E7%9A%84subword%E7%AE%97%E6%B3%95/wordpiece2.jpg"></p>
<p>这不就是两个token之间的互信息嘛！所以wordpiece和BPE的核心区别就在于wordpiece是按token间的互信息来进行合并而BPE是按照token一同出现的频率来合并的。</p>
<p>wordpiece算法中subword词表的学习跟BPE也差不多:</p>
<ul>
<li>准备语料，分解成最小单元，比如英文中26个字母加上各种符号，作为原始词表</li>
<li>利用上述语料训练语言模型</li>
<li>从所有可能的subword单元中选择加入语言模型后能最大程度地增加训练数据概率的单元作为新的单元</li>
<li>重复上步骤，直至词表大小达到设定值或概率增量低于某一阈值</li>
</ul>
<h2 id="2-3-unigram-language-model"><a href="#2-3-unigram-language-model" class="headerlink" title="2.3 unigram language model"></a>2.3 unigram language model</h2><p>语言模型作为NLP的大厦根基，也是unigram分词的基础。在wordpiece算法中，其实已经用到了language modeling，在选择token进行合并的时候目标就是能提高句子的likelihood。而unigram分词则更进一步，直接以最大化句子的likelihood为目标来直接构建整个词表。</p>
<p>首先，了解一下怎么样在给定词表的条件下最大化句子的likelihood。<br>给定词表及对应概率值: {“你”:0.18, “们”:0.16, “好”:0.18, “你们”:0.15}，对句子”你们好“进行分词:</p>
<ul>
<li>划分为”你@@” “们@@” “好” 的概率为 0.18*0.16*0.18=0.005184</li>
<li>划分为”你们@@” “好@@” 的概率为 0.15*0.18=0.027</li>
</ul>
<p>明显看出后一种分词方式要比前一种好，当然在真实的案例下词表可能有几万个token，直接罗列各种组合的概率显然不可能，所以需要用到Viterbi算法。因此在给定词表的情况下，可以 <strong>1.计算每个token对应的概率；2.找到一个句子最好的分词方式</strong></p>
<p>但是在词表没有确定的情况下，同时要优化词表和词表里每个token的概率很难做到。unigram分词使用逐步迭代的方式来求解，具体步骤如下：</p>
<ul>
<li>首先初始化一个很大的词表</li>
<li>重复以下步骤直到词表数量减少到预先设定的阈值：<ul>
<li>保持词表不变，用EM算法来求解每个token的概率</li>
<li>对于每一个token，计算如果把这个token从词表中移除而导致的likelihood减少值，作为这个token的loss</li>
<li>按loss从大到小排序，保留前n%（原文中为80%）的token。</li>
</ul>
</li>
</ul>
<p>初始化词表可以用不同的方法，一个比较直接的办法就是用所有长度为1的token加上高频出现的ngram来作为起始词表。</p>
<h1 id="3-常用的分词开源实现"><a href="#3-常用的分词开源实现" class="headerlink" title="3. 常用的分词开源实现"></a>3. 常用的分词开源实现</h1><blockquote>
<p>在自然语言处理中，常见的subword算法开源实现以下三种，这里主要讲述<strong>subword-nmt</strong>的使用</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/rsennrich/subword-nmt">subword-nmt</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/glample/fastBPE">fastBPE</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/google/sentencepiece">sentencepiece</a></li>
</ul>
</blockquote>
<h2 id="3-1-subword-nmt"><a href="#3-1-subword-nmt" class="headerlink" title="3.1 subword-nmt"></a>3.1 subword-nmt</h2><h3 id="3-1-1-简介"><a href="#3-1-1-简介" class="headerlink" title="3.1.1 简介"></a>3.1.1 简介</h3><p>subword-nmt的项目地址为: <a target="_blank" rel="noopener" href="https://github.com/rsennrich/subword-nmt%E3%80%82">https://github.com/rsennrich/subword-nmt。</a><br>文件主要组成部分：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── __init__.py</span><br><span class="line">├── apply_bpe.py</span><br><span class="line">├── bpe_toy.py</span><br><span class="line">├── chrF.py</span><br><span class="line">├── get_vocab.py</span><br><span class="line">├── learn_bpe.py</span><br><span class="line">├── learn_joint_bpe_and_vocab.py</span><br><span class="line">├── segment_char_ngrams.py</span><br><span class="line">└── subword_nmt.py</span><br></pre></td></tr></table></figure>
<p>subword-nmt可通过pip进行安装</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install subword-nmt</span><br><span class="line">or</span><br><span class="line">pip install https://github.com/rsennrich/subword-nmt/archive/master.zip</span><br></pre></td></tr></table></figure>
<p>也可直接<code>git clone</code>直接食用：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python learn_bpe.py [-options]</span><br></pre></td></tr></table></figure>
<p>由于翻译项目动辄几百万的语料，subword-nmt的处理速度要比fastBPE慢很多，此时配合pypy和多进程处理食用更佳。</p>
<h3 id="3-1-2-使用方法"><a href="#3-1-2-使用方法" class="headerlink" title="3.1.2. 使用方法"></a>3.1.2. 使用方法</h3><p>在subword-nmt中，比较关键的脚本有<code>learn_bpe.py</code>、<code>apply_bpe.py</code>和<code>get_vocab.py</code>。一般来说，使用BPE进行分词有以下几个步骤：learn_bpe、apply_bpe和get_vocab。如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 学习codes和vocab</span><br><span class="line">cat &#123;train_file&#125;.L1 &#123;train_file&#125;.L2 | subword-nmt learn-bpe -s &#123;num_operations&#125; -o &#123;codes_file&#125;</span><br><span class="line">subword-nmt apply-bpe -c &#123;codes_file&#125; &lt; &#123;train_file&#125;.L1 | subword-nmt get-vocab &gt; &#123;vocab_file&#125;.L1</span><br><span class="line">subword-nmt apply-bpe -c &#123;codes_file&#125; &lt; &#123;train_file&#125;.L2 | subword-nmt get-vocab &gt; &#123;vocab_file&#125;.L2</span><br><span class="line">or</span><br><span class="line">subword-nmt learn-joint-bpe-and-vocab --input &#123;train_file&#125;.L1 &#123;train_file&#125;.L2 -s &#123;num_operations&#125; -o &#123;codes_file&#125; --write-vocabulary &#123;vocab_file&#125;.L1 &#123;vocab_file&#125;.L2</span><br><span class="line"></span><br><span class="line"># 对训练语料进行分词</span><br><span class="line">subword-nmt apply-bpe -c &#123;codes_file&#125; --vocabulary &#123;vocab_file&#125;.L1 --vocabulary-threshold 50 &lt; &#123;train_file&#125;.L1 &gt; &#123;train_file&#125;.BPE.L1</span><br><span class="line">subword-nmt apply-bpe -c &#123;codes_file&#125; --vocabulary &#123;vocab_file&#125;.L2 --vocabulary-threshold 50 &lt; &#123;train_file&#125;.L2 &gt; &#123;train_file&#125;.BPE.L2</span><br><span class="line"></span><br><span class="line"># 对测试语料进行分词</span><br><span class="line">subword-nmt apply-bpe -c &#123;codes_file&#125; --vocabulary &#123;vocab_file&#125;.L1 --vocabulary-threshold 50 &lt; &#123;test_file&#125;.L1 &gt; &#123;test_file&#125;.BPE.L1</span><br><span class="line"></span><br><span class="line"># 得到词典，这里的方法有多种</span><br><span class="line">cat &#123;train_file&#125;.BPE.L1 &#123;train_file&#125;.BPE.L2 | subword-nmt get-vocab &gt; &#123;vocab_file&#125;</span><br><span class="line">or</span><br><span class="line">nematus/data/build_dictionary.py &#123;train_file&#125;.BPE.L1 &#123;train_file&#125;.BPE.L2 # nematus中的实现</span><br></pre></td></tr></table></figure>
<p>此外，BPE-dropout是一种为分词过程提供随机性的处理方式。与BPE相比，BPE-dropout在对训练的平行语料进行apply_bpe的过程中，设定一定概率让融合(merge)不通过，这样对于同一个单词，也会出现不同的拆分过程。可以通过调整 <code>--dropout 0.1</code>这个参数为<code>apply-bpe</code>这个过程增加随机性。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/translation/" rel="tag"># translation</a>
              <a href="/tags/subword/" rel="tag"># subword</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/2020/05/22/ODPS%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8C%97/" rel="next" title="ODPS操作指北(1):数据开发实用语法">
      ODPS操作指北(1):数据开发实用语法 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-word%E3%80%81subword%E5%92%8Ccharacter"><span class="nav-text">1.word、subword和character</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-3%E7%A7%8Dsubword%E7%AE%97%E6%B3%95"><span class="nav-text">2. 3种subword算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-Byte-Pair-Encoding"><span class="nav-text">2.1 Byte Pair Encoding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-wordpiece"><span class="nav-text">2.2 wordpiece</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-unigram-language-model"><span class="nav-text">2.3 unigram language model</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-%E5%B8%B8%E7%94%A8%E7%9A%84%E5%88%86%E8%AF%8D%E5%BC%80%E6%BA%90%E5%AE%9E%E7%8E%B0"><span class="nav-text">3. 常用的分词开源实现</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-subword-nmt"><span class="nav-text">3.1 subword-nmt</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-%E7%AE%80%E4%BB%8B"><span class="nav-text">3.1.1 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="nav-text">3.1.2. 使用方法</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="zhangjf"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">zhangjf</p>
  <div class="site-description" itemprop="description">Even night drunk with you for real.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yourname" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?hl=zh-CN&user=K7spOEYAAAAJ" title="Scholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl&#x3D;zh-CN&amp;user&#x3D;K7spOEYAAAAJ" rel="noopener" target="_blank"><i class="fab fa-google fa-fw"></i>Scholar</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zhangjf</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">6k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">6 mins.</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/three-waves.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>
    <script defer src="/lib/three/canvas_sphere.min.js"></script>


  















  

  

</body>
</html>
